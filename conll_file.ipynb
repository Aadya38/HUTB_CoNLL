{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZpAvYqtZcQE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple, Any\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s')\n",
        "\n",
        "\n",
        "def parse_variant_id(var_id: str, prefix: str = 'file-katl_e_bayan-1507111042') -> Tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Parse a variant ID into its reference and sub-variant numbers for numerical sorting.\n",
        "\n",
        "    Args:\n",
        "        var_id (str): Variant ID (e.g., 'file-katl_e_bayan-1507111042__1.2').\n",
        "        prefix (str): Prefix to remove from the ID.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[int, int]: Tuple of (reference_id, sub_id).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not var_id.startswith(prefix):\n",
        "            return (9999, 0)  # Invalid IDs go to the end\n",
        "        # Extract the number part, e.g., '1.2' from 'file-katl_e_bayan-1507111042__1.2'\n",
        "        number_part = var_id[len(prefix) + 2:]  # Skip prefix and '__'\n",
        "        ref_id, sub_id = map(int, number_part.split('.'))\n",
        "        return (ref_id, sub_id)\n",
        "    except (ValueError, IndexError):\n",
        "        logging.warning(f\"Invalid variant ID format: {var_id}\")\n",
        "        return (9999, 0)\n",
        "\n",
        "# Function to read variant sentences from CSV\n",
        "def read_variant_sentences(variant_file: str) -> Dict[str, List[List[Tuple[str, int]]]]:\n",
        "    \"\"\"\n",
        "    Read variant sentences from a CSV file and return a dictionary of sentence IDs to word-index pairs.\n",
        "\n",
        "    Args:\n",
        "        variant_file (str): Path to the variant CSV file.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, List[List[Tuple[str, int]]]]: Dictionary mapping sentence IDs to lists of word-index pairs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read raw file for debugging\n",
        "        with open(variant_file, 'r', encoding='utf-8') as f:\n",
        "            raw_content = f.read()\n",
        "            logging.info(f\"Raw content of {variant_file}:\\n{raw_content[:500]}...\")\n",
        "\n",
        "        # Try comma separator\n",
        "        df = pd.read_csv(variant_file, sep=',', encoding='utf-8', header=None, na_values=[''])\n",
        "        logging.info(f\"Parsed {variant_file} with comma separator. Columns: {df.shape[1]}\")\n",
        "        logging.debug(f\"First few rows:\\n{df.head().to_string()}\")\n",
        "\n",
        "        # Handle different column counts\n",
        "        if df.shape[1] == 4:\n",
        "            df.columns = ['index', 'sentence_id', 'sentence', 'extra']\n",
        "        elif df.shape[1] == 3:\n",
        "            df.columns = ['index', 'sentence_id', 'sentence']\n",
        "        elif df.shape[1] == 2:\n",
        "            df.columns = ['sentence_id', 'sentence']\n",
        "            df['index'] = range(len(df))\n",
        "        elif df.shape[1] >= 8:\n",
        "            df.columns = ['index', 'sentence_id', 'sentence'] + [f'extra{i}' for i in range(df.shape[1] - 3)]\n",
        "        else:\n",
        "            logging.warning(f\"Unexpected number of columns in {variant_file}: {df.shape[1]}. Assuming index, sentence_id, sentence, extra...\")\n",
        "            df.columns = ['index', 'sentence_id', 'sentence'] + [f'extra{i}' for i in range(df.shape[1] - 3)]\n",
        "\n",
        "        # Try tab separator if sentence column is all NaN\n",
        "        if 'sentence' in df.columns and df['sentence'].isna().all():\n",
        "            logging.warning(\"Comma separator failed, trying tab separator\")\n",
        "            df = pd.read_csv(variant_file, sep='\\t', encoding='utf-8', header=None, na_values=[''])\n",
        "            logging.info(f\"Parsed {variant_file} with tab separator. Columns: {df.shape[1]}\")\n",
        "            if df.shape[1] == 4:\n",
        "                df.columns = ['index', 'sentence_id', 'sentence', 'extra']\n",
        "            elif df.shape[1] == 3:\n",
        "                df.columns = ['index', 'sentence_id', 'sentence']\n",
        "            elif df.shape[1] == 2:\n",
        "                df.columns = ['sentence_id', 'sentence']\n",
        "                df['index'] = range(len(df))\n",
        "            else:\n",
        "                logging.warning(f\"Unexpected number of columns with tab separator in {variant_file}: {df.shape[1]}. Assuming index, sentence_id, sentence, extra...\")\n",
        "                df.columns = ['index', 'sentence_id', 'sentence'] + [f'extra{i}' for i in range(df.shape[1] - 3)]\n",
        "\n",
        "        variants = defaultdict(list)\n",
        "        for _, row in df.iterrows():\n",
        "            if pd.isna(row['sentence_id']) or ('sentence' in df.columns and pd.isna(row['sentence'])):\n",
        "                logging.warning(f\"Skipping empty or invalid row: {row}\")\n",
        "                continue\n",
        "            sentence_id = str(row['sentence_id']).strip()\n",
        "            words = str(row.get('sentence', '')).strip().split() if 'sentence' in df.columns else []\n",
        "            word_indices = []\n",
        "            for word in words:\n",
        "                if word == '__NULL__':\n",
        "                    continue\n",
        "                if '_' in word and word != 'ред':\n",
        "                    try:\n",
        "                        w, idx = word.split('_')\n",
        "                        word_indices.append((w, int(idx)))\n",
        "                    except ValueError:\n",
        "                        logging.warning(f\"Invalid index in word: {word}\")\n",
        "                        word_indices.append((word, None))\n",
        "                else:\n",
        "                    word_indices.append((word, None))\n",
        "            variants[sentence_id].append(word_indices)\n",
        "        return variants\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error reading variant file {variant_file}: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to read CoNLL text file\n",
        "def read_conll_file(conll_file: str) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    Read a CoNLL file and return a list of sentences, where each sentence is a list of token rows.\n",
        "\n",
        "    Args:\n",
        "        conll_file (str): Path to the CoNLL file.\n",
        "\n",
        "    Returns:\n",
        "        List[List[str]]: List of sentences, each containing rows of CoNLL columns.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        sentences = []\n",
        "        current_sentence = []\n",
        "        df = pd.read_csv(conll_file, sep='\\t', encoding='utf-8', header=None, na_values=[''], comment='#')\n",
        "        for _, row in df.iterrows():\n",
        "            if row.isna().all() or str(row[0]).strip() == '':\n",
        "                if current_sentence:\n",
        "                    sentences.append(current_sentence)\n",
        "                    current_sentence = []\n",
        "            else:\n",
        "                current_sentence.append(row.tolist())\n",
        "        if current_sentence:\n",
        "            sentences.append(current_sentence)\n",
        "        logging.info(f\"Read {len(sentences)} sentences from {conll_file}\")\n",
        "        return sentences\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error reading CoNLL file {conll_file}: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to generate variant CoNLL entries\n",
        "def generate_variant_conll(reference_sentence: List[List[str]], variant_words: List[Tuple[str, int]], sentence_id: str) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    Generate CoNLL entries for a variant sentence, mapping variant words to reference annotations.\n",
        "\n",
        "    Args:\n",
        "        reference_sentence (List[List[str]]): CoNLL rows of the reference sentence.\n",
        "        variant_words (List[Tuple[str, int]]): List of (word, index) tuples for the variant.\n",
        "        sentence_id (str): ID of the variant sentence.\n",
        "\n",
        "    Returns:\n",
        "        List[List[str]]: List of CoNLL rows for the variant.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not reference_sentence or not variant_words:\n",
        "            logging.warning(f\"Empty reference or variant words for {sentence_id}\")\n",
        "            return []\n",
        "\n",
        "        ref_dict = {}\n",
        "        for row in reference_sentence:\n",
        "            if len(row) < 6:\n",
        "                logging.warning(f\"Skipping malformed CoNLL row in {sentence_id}: {row}\")\n",
        "                continue\n",
        "            try:\n",
        "                index = int(row[0])\n",
        "                word = row[1] if row[1] else '_'\n",
        "                pos_tag = row[3] if row[3] else 'UNK'\n",
        "                head = int(row[4]) if row[4] != '0' else 0\n",
        "                rel = row[5] if row[5] else '_'\n",
        "                ref_dict[index] = {'word': word, 'pos_tag': pos_tag, 'head': head, 'rel': rel}\n",
        "            except (ValueError, IndexError) as e:\n",
        "                logging.warning(f\"Error processing CoNLL row in {sentence_id}: {row}, {e}\")\n",
        "                continue\n",
        "\n",
        "        variant_conll = []\n",
        "        new_index = 1\n",
        "        word_indices = []\n",
        "        for word, idx in variant_words:\n",
        "            word_indices.append((word, idx, new_index))\n",
        "            new_index += 1\n",
        "\n",
        "        for word, idx, new_index in word_indices:\n",
        "            if idx is None:\n",
        "                if word == 'ред':\n",
        "                    for row in reference_sentence:\n",
        "                        if len(row) >= 6 and row[1] == 'ред':\n",
        "                            variant_conll.append([new_index, word, row[3], row[4], row[5]])\n",
        "                            break\n",
        "                    else:\n",
        "                        logging.warning(f\"Punctuation 'ред' not found in reference for {sentence_id}\")\n",
        "                        variant_conll.append([new_index, word, 'SYM', '0', 'rsym'])\n",
        "                else:\n",
        "                    for row in reference_sentence:\n",
        "                        if len(row) >= 6 and row[1] == word:\n",
        "                            variant_conll.append([new_index, word, row[3], row[4], row[5]])\n",
        "                            break\n",
        "                    else:\n",
        "                        logging.warning(f\"Word {word} not found in reference for {sentence_id}, using defaults\")\n",
        "                        variant_conll.append([new_index, word, 'UNK', '0', 'UNK'])\n",
        "            else:\n",
        "                ref_entry = ref_dict.get(idx, {'word': word, 'pos_tag': 'UNK', 'head': 0, 'rel': 'UNK'})\n",
        "                new_head = 0\n",
        "                if ref_entry['head'] != 0:\n",
        "                    for _, ref_idx, variant_idx in word_indices:\n",
        "                        if ref_idx == ref_entry['head']:\n",
        "                            new_head = variant_idx\n",
        "                            break\n",
        "                    if new_head == 0:\n",
        "                        logging.warning(f\"Head index {ref_entry['head']} not found in variant {sentence_id}, resetting to 0\")\n",
        "                logging.debug(f\"Mapping {word} (ref_idx={idx}) to head={new_head} in {sentence_id}\")\n",
        "                variant_conll.append([new_index, word, ref_entry['pos_tag'], str(new_head), ref_entry['rel']])\n",
        "\n",
        "        return variant_conll\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating CoNLL for {sentence_id}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Main function to process files and generate output CSV\n",
        "def generate_conll_output(conll_file: str, variant_file: str, output_file: str) -> None:\n",
        "    \"\"\"\n",
        "    Process CoNLL and variant files to generate a combined CoNLL output.\n",
        "\n",
        "    Args:\n",
        "        conll_file (str): Path to the CoNLL input file.\n",
        "        variant_file (str): Path to the variant CSV file.\n",
        "        output_file (str): Path to the output CoNLL file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read input files\n",
        "        sentences = read_conll_file(conll_file)\n",
        "        variants = read_variant_sentences(variant_file)\n",
        "\n",
        "        # Open output file\n",
        "        with open(output_file, 'w', encoding='utf-8', newline='') as f:\n",
        "            writer = csv.writer(f, delimiter='\\t')\n",
        "            writer.writerow(['word', 'pos_tag', 'head', 'rel'])\n",
        "\n",
        "            base_prefix = 'file-katl_e_bayan-1507111042'\n",
        "            # Group variant IDs by reference sentence\n",
        "            variant_groups = defaultdict(list)\n",
        "            for var_id in variants.keys():\n",
        "                if var_id.startswith(base_prefix):\n",
        "                    ref_id, sub_id = parse_variant_id(var_id, base_prefix)\n",
        "                    if sub_id == 0:  # Reference sentence (e.g., 1.0, 2.0)\n",
        "                        variant_groups[ref_id].append((var_id, True))\n",
        "                    else:  # Sub-variant (e.g., 1.1, 1.2)\n",
        "                        variant_groups[ref_id].append((var_id, False))\n",
        "\n",
        "            # Process each reference sentence and its variants in numerical order\n",
        "            for ref_id in sorted(variant_groups.keys()):\n",
        "                # Sort variants: reference first (X.0), then sub-variants (X.1, X.2, ...)\n",
        "                group = sorted(variant_groups[ref_id], key=lambda x: parse_variant_id(x[0], base_prefix)[1])\n",
        "\n",
        "                # Map reference ID to sentence index (e.g., 1.0 -> sentences[0], 2.0 -> sentences[1])\n",
        "                try:\n",
        "                    sentence_idx = ref_id - 1  # Assume 1.0 is sentences[0], 2.0 is sentences[1], etc.\n",
        "                    if sentence_idx < 0 or sentence_idx >= len(sentences):\n",
        "                        logging.warning(f\"No reference sentence found for ref_id {ref_id}. Skipping.\")\n",
        "                        continue\n",
        "                    ref_sentence = sentences[sentence_idx]\n",
        "                except IndexError:\n",
        "                    logging.warning(f\"Reference sentence index {sentence_idx} out of range for ref_id {ref_id}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                for var_id, is_reference in group:\n",
        "                    try:\n",
        "                        writer.writerow([f'# {var_id}'])\n",
        "                        # Use reference sentence for both reference and variants\n",
        "                        if is_reference:\n",
        "                            # Write reference sentence directly\n",
        "                            for row in ref_sentence:\n",
        "                                if len(row) >= 6:\n",
        "                                    writer.writerow([row[1], row[3], row[4], row[5]])\n",
        "                                else:\n",
        "                                    logging.warning(f\"Skipping malformed row in {var_id}: {row}\")\n",
        "                        else:\n",
        "                            # Generate variant CoNLL\n",
        "                            variant_conll = generate_variant_conll(ref_sentence, variants[var_id][0], var_id)\n",
        "                            if not variant_conll:\n",
        "                                logging.warning(f\"No CoNLL data generated for {var_id}\")\n",
        "                                continue\n",
        "                            for row in variant_conll:\n",
        "                                if len(row) >= 5:\n",
        "                                    writer.writerow([row[1], row[2], row[3], row[4]])\n",
        "                                else:\n",
        "                                    logging.warning(f\"Invalid row in {var_id}: {row}\")\n",
        "                        writer.writerow([])\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Error processing variant {var_id}: {e}\")\n",
        "                        continue\n",
        "\n",
        "        logging.info(f\"Output written to {output_file}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating output: {e}\")\n",
        "        raise\n",
        "\n",
        "# File paths\n",
        "conll_file = 'hutb_conll.txt'\n",
        "variant_file = 'hutb_katle_final.csv'\n",
        "output_file = 'katl_e_bayan_conll_final.csv'\n",
        "\n",
        "# Run the function\n",
        "if __name__ == \"__main__\":\n",
        "    generate_conll_output(conll_file, variant_file, output_file)"
      ]
    }
  ]
}